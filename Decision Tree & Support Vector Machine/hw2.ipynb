{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS145 Howework 2\n",
    "\n",
    "\n",
    "<span style=\"color:red\"> **Important Note:** </span>\n",
    "HW2 is due on **11:59 PM PT, Oct 30 (Friday, Week 4)**. Please submit through GradeScope. \n",
    "\n",
    "## Print Out Your Name and UID\n",
    "\n",
    "<span style=\"color:blue\"> **Name: Rui Deng, UID: 205123245** </span>\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "You need to first create HW2 conda environment by the given `cs145hw2.yml` file, which provides the name and necessary packages for this tasks. If you have `conda` properly installed, you may create, activate or deactivate by the following commands:\n",
    "\n",
    "```\n",
    "conda env create -f cs145hw2.yml\n",
    "conda activate hw1\n",
    "conda deactivate\n",
    "```\n",
    "OR \n",
    "\n",
    "```\n",
    "conda env create --name NAMEOFYOURCHOICE -f cs145hw2.yml \n",
    "conda activate NAMEOFYOURCHOICE\n",
    "conda deactivate\n",
    "```\n",
    "To view the list of your environments, use the following command:\n",
    "```\n",
    "conda env list\n",
    "```\n",
    "\n",
    "More useful information about managing environments can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).\n",
    "\n",
    "You may also quickly review the usage of basic Python and Numpy package, if needed in coding for matrix operations.\n",
    "\n",
    "In this notebook, you must not delete any code cells in this notebook. If you change any code outside the blocks (such as some important hyperparameters) that you are allowed to edit (between STRART/END YOUR CODE HERE), you need to highlight these changes. You may add some additional cells to help explain your results and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys \n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can successfully run the code above, there will be no problem for environment setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision trees\n",
    "This workbook will walk you through a decision tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Attribute selection measures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification models, misclassification rate is usually used as the final performance measurement. However, for classification trees, when selecting which attribute to split, measurements people often use includes information gain, gain ratio, and Gini index. Let's investigate these different measurements through the following problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: below shows how to calculate the misclassification rate of a classification tree with $N$ total data points, $K$ classes of the value we want to predict, and $M$ leaf nodes. \n",
    "\n",
    "In a node $m$, $m = 1, ..., M$, let's denote the number of data points using $N_m$, and the number of data points in class k as $N_{mk}$, so the class prediction under majority vote is $j = argmax_k N_{mk}$. The misclassification rate of this node m is $R_m = 1 - \\frac{N_{mj}}{N_m}$. The total misclassification rate of the tree will be $R = \\frac{\\sum_{m=1}^M R_m * N_m}{N}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Questions**\n",
    "\n",
    "<span style=\"color:red\"> Note: this question is a pure \"question answer\" problem. You don't need to do any coding. </span>\n",
    "\n",
    "Suppose our dataset includes a total of 800 people with 400 males and 400 females, and our goal is to do gender classification. Consider two different possible attributes we can split on in a decision tree model. Split on the first attribute results in a node11 with 300 male and 100 female, and a node12 with 100 male and 300 female. Split on the second attribute results in in a node21 with 400 male and 200 female, and a node22 with 200 female only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which split do you prefer when the measurement is misclassifcation rate and why?\n",
    "2. What is the entropy in each of these four node? \n",
    "3. What is the information gain of each of the two splits?\n",
    "4. Which split do you prefer if the measurement is information gain. Do you see why it is an uncertainty or impurity measurement?\n",
    "5. What is the gain ratio (normalized information gain) of each of the two splits? Which split do you prefer under this measurement. Do you get the same conclusion as information gain? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> Note: you can use several code cells to help you compute the results and answer the questions. Again you don't need to do any coding. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Please type your answer here! </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 1\n",
    "\n",
    "By the definition of misclassification rate, we can see that it can be calculated as the number of misclassified samples divided by the total sample number.\n",
    "Thus, misclassification for the split on the first attribute is: (100 females are misclassified on node11, 100 males are misclassified on node 12)\n",
    "$$ \\frac {(100 + 100)} {800} = 0.25 $$\n",
    "Misclassification for the split on the second attribute: (200 females are misclassified on node21)\n",
    "$$ \\frac {200} {800} = 0.25 $$\n",
    "Thus, there is no different between two splits if we only consider the misclassification rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 2\n",
    "\n",
    "We know that:\n",
    "$$ Entropy = Info(D) = - \\sum_{i=1}^m p_i log_2(p_i) $$\n",
    "So entropy for node11 is:\n",
    "$$ Info(node11) = - \\frac {3} {4} log (\\frac {3} {4}) - \\frac {1} {4} log (\\frac {1} {4}) = 0.8112781 $$\n",
    "Similarly,\n",
    "$$ Info(node12) = - \\frac {3} {4} log (\\frac {3} {4}) - \\frac {1} {4} log (\\frac {1} {4}) = 0.8112781 $$\n",
    "\n",
    "$$ Info(node21) = - \\frac {2} {3} log (\\frac {2} {3}) - \\frac {1} {3} log (\\frac {1} {3}) = 0.9182958 $$\n",
    "\n",
    "$$ Info(node22) = - 1 log (1) = 0 $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 3\n",
    "\n",
    "We first calculate the entropy of dataset:\n",
    "$$ Info(root) = - \\frac {1} {2} log (\\frac {1} {2}) - \\frac {1} {2} log (\\frac {1} {2}) = 1 $$\n",
    "Then, the information gain for the first split is:\n",
    "$$ Gain(first) = 1 - (\\frac {400} {800} * 0.8112781 + \\frac {400} {800} * 0.8112781) = 0.187129 $$\n",
    "Similarly, the information gain for the second split is:\n",
    "$$ Gain(second) = 1 - (\\frac {600} {800} * 0.0.9182958 + \\frac {200} {800} * 0) = 0.31127813 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 4\n",
    "\n",
    "We should choose the split on the second attribute, since it gives us the most information gain. \n",
    "\n",
    "The entropy is a measure of uncertainty, since when we have 400 females and 400 males in the root node, we have an entropy of 1, saying that we are totally unsure about the classification; conversely, when we have only 200 females in the node22, then the entropy is 0, meaning that there is no uncertainty or impurity in this node, and we are confident that this node should be classified to the female class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 5\n",
    "\n",
    "By the formula, the split info of the first split is:\n",
    "$$ SplitInfo(first) = - \\frac {1} {2} log (\\frac {1} {2}) - \\frac {1} {2} log (\\frac {1} {2}) = 1 $$\n",
    "And the gain ratio of the split split is therefore:\n",
    "$$ GainRatio(first) = \\frac {Gain(first)} {SplitInfo(first)} = 0.18729$$\n",
    "Similarly:\n",
    "$$ SplitInfo(second) = - \\frac {3} {4} log (\\frac {3} {4}) - \\frac {1} {4} log (\\frac {1} {4}) = 0.8112781 $$\n",
    "\n",
    "$$ GainRatio(second) = \\frac {Gain(second)} {SplitInfo(second)} = 0.3836885$$\n",
    "\n",
    "Still, we will choose the second attribute to split, since it gives us the most gain ratio, and this result is the same as information gain.\n",
    "\n",
    "We know that information gain is biased towards attribute with multiple values, but in this case, we only have 2 values for each attribute, so the gain ratio does not give us a different final choice of attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Coding decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to use the decision tree model to predict the the animal `type` class of the `zoo` dataset. The dataset has been preprocessed and splited into `decision-tree-train.csv` and `decision-tree-test.csv` for you.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (80, 17)\n",
      "Testing data shape: (21, 17)\n"
     ]
    }
   ],
   "source": [
    "from hw2code.decision_tree import DecisionTree\n",
    "mytree = DecisionTree()\n",
    "mytree.load_data('./data/decision-tree-train.csv','./data/decision-tree-test.csv')\n",
    "# As a sanity check, we print out the size of the training data (80, 17) and testing data (21, 17)\n",
    "print('Training data shape: ', mytree.train_data.shape)\n",
    "print('Testing data shape:', mytree.test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Infomation gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the `make_tree` and `compute_info_gain` function in `decision_tree.py`. \n",
    "\n",
    "Train you model using `info_gain` measure to classify `type` and print the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_feature is:  legs\n",
      "best_feature is:  fins\n",
      "best_feature is:  toothed\n",
      "best_feature is:  eggs\n",
      "best_feature is:  hair\n",
      "best_feature is:  hair\n",
      "best_feature is:  toothed\n",
      "best_feature is:  aquatic\n",
      "Test accuracy is:  0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "mytree = DecisionTree()\n",
    "mytree.load_data('./data/decision-tree-train.csv','./data/decision-tree-test.csv')\n",
    "test_acc= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "mytree.train('type', 'info_gain')\n",
    "test_acc = mytree.test('type')\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Test accuracy is: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Gain ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the `compute_gain_ratio` function in `decision_tree.py`. \n",
    "\n",
    "Train you model using `gain_ratio` measure to classify `type` and print the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_feature is:  feathers\n",
      "best_feature is:  backbone\n",
      "best_feature is:  airborne\n",
      "best_feature is:  predator\n",
      "best_feature is:  milk\n",
      "best_feature is:  fins\n",
      "best_feature is:  legs\n",
      "Test accuracy is:  0.8095238095238095\n"
     ]
    }
   ],
   "source": [
    "mytree = DecisionTree()\n",
    "mytree.load_data('./data/decision-tree-train.csv','./data/decision-tree-test.csv')\n",
    "test_acc = 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "mytree.train('type', 'gain_ratio')\n",
    "test_acc = mytree.test('type')\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Test accuracy is: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "Which measure do you like the most and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**\n",
    "\n",
    "We know that information gain is biased towards attributes with a large number of values, whereas the gain ratio tends to prefer unbalanced splits in which one partition is much smaller than others. Then, we need to examine our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hair:  [43 37]\n",
      "feathers:  [64 16]\n",
      "eggs:  [37 43]\n",
      "milk:  [44 36]\n",
      "airborne:  [62 18]\n",
      "aquatic:  [50 30]\n",
      "predator:  [31 49]\n",
      "toothed:  [30 50]\n",
      "backbone:  [13 67]\n",
      "breathes:  [17 63]\n",
      "venomous:  [74  6]\n",
      "fins:  [66 14]\n",
      "legs:  [17 20 33  8  2]\n",
      "tail:  [20 60]\n",
      "domistic:  [67 13]\n",
      "catsize:  [43 37]\n",
      "type:  [36 16  2 10  3  6  7]\n"
     ]
    }
   ],
   "source": [
    "mytree = DecisionTree()\n",
    "mytree.load_data('./data/decision-tree-train.csv','./data/decision-tree-test.csv')\n",
    "features = mytree.train_data.columns.values\n",
    "for feature in features:\n",
    "    vals, counts = np.unique(mytree.train_data[feature], return_counts=True)\n",
    "    print(\"%s: \" % feature, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are only two attributes with multiple number of values other than two, but there are lots of attributes, such as venomous, tail, legs, and demoistic, that has unbalanced splits. Therefore, in this case, using information gain gives us a better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SVM \n",
    "This workbook will walk you through a SVM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Support vectors and decision boundary\n",
    "\n",
    "<span style=\"color:red\"> Note: for this question you can work entirely in the Jupyter Notebook, no need to edit any .py files. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider classifying the following 20 data points in the 2-d plane with class label y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.52</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.48</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.46</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x1    x2  y\n",
       "0  0.52 -1.00  1\n",
       "1  0.91  0.32  1\n",
       "2 -1.48  1.23  1\n",
       "3  0.01  1.44  1\n",
       "4 -0.46 -0.37  1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.read_csv('data/svm-2d-data.csv')\n",
    "ds.head()\n",
    "# This command above will print out the first five data points\n",
    "# in the dataset with column names as \"x1\", \"x2\" and \"y\"\n",
    "# You may use command \"ds\" to show the entire dataset, which contains 20 data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose by solving the dual form of the quadratic programming of svm, we can derive the $\\alpha_i$â€™s for each data point as follows: Among $j=0,1,\\cdots, 19$ (note that the index starts from 0), $\\alpha_1$ = 0.5084, $\\alpha_5$ = 0.4625, $\\alpha_{17}$ = 0.9709, and $\\alpha_j$ = 0 for all other $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which vectors in the training points are support vectors?\n",
    "2. What is the normal vector of the hyperplane $w$? \n",
    "3. What is the bias $b$? \n",
    "4. With the parameters $w$ and $b$, we can now use our SVM to do predictions. What is predicted label of $x_{new} = (2,-0.5)$? Write out your $f(x_{new})$.\n",
    "5. A plot of the data points has been generated for you. Please change the `support_vec` variable such that only the support vectors are indicated by red circles. Please also fill in the code to draw the decision boundary. Does your prediction of part 4 seems right visually on the plot?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> Note: you can use several code cells to help you compute the results and answer the questions. Again you don't need to edit any .py files. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Please type your answer here! </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 1\n",
    "\n",
    "The support vectors are data points with non-zero $ \\alpha $ values. In this case, data point 1, 5, 17 are the only three support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 2\n",
    "\n",
    "The solution of SVM gives us the normal vector as:\n",
    "$$ w = \\sum {a_i y_i x_i} $$\n",
    "Thus, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.338076 -0.388998]\n"
     ]
    }
   ],
   "source": [
    "w = 0.5084 * ds.iloc[1][2] * ds.iloc[1][:2].to_numpy() + 0.4625 * ds.iloc[5][2] * ds.iloc[5][:2].to_numpy() + \\\n",
    "0.9709 * ds.iloc[17][2] * ds.iloc[17][:2].to_numpy()\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the normal vector is  $ w = [-1.338076, -0.388998] $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 3\n",
    "\n",
    "The solution of SVM gives us the bias term as:\n",
    "$$  b = y_k - w^T x_k $$ for any $x_k$ with non-zero $\\alpha_k$ term.\n",
    "Thus, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.342136106666666\n"
     ]
    }
   ],
   "source": [
    "b = ds.iloc[1][2] - w.dot(ds.iloc[1][:2].to_numpy())\n",
    "b += ds.iloc[5][2] - w.dot(ds.iloc[5][:2].to_numpy())\n",
    "b += ds.iloc[17][2] - w.dot(ds.iloc[17][:2].to_numpy())\n",
    "b /= 3\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we take the average of three possible bias terms, and the bias is therefore 2.3421361."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 4\n",
    "\n",
    "We know that\n",
    "$$ f(x_{new}) = w^T x_k + b $$\n",
    "So we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1395168933333335\n"
     ]
    }
   ],
   "source": [
    "x = np.array([2, -0.5])\n",
    "f = w.dot(x) + b\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have $ f(x_{new}) = -0.139516893 $ is less than 0, so we predict that it belongs to y = -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# answer 5\n",
    "x1_range = np.arange(-2, 5, 0.5)\n",
    "x2_range = np.arange(-2, 4., 0.5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax = fig.gca()\n",
    "ax.set_xticks(x1_range)\n",
    "ax.set_yticks(x2_range)\n",
    "ax.grid()\n",
    "ax.scatter(ds['x1'], ds['x2'], c=ds['y'])\n",
    "\n",
    "support_vec = ds\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "support_vec = ds.iloc[[1, 5, 17], :]\n",
    "xs = np.linspace(1, 2, 100)\n",
    "ys = - w[0] / w[1] * xs - b / w[1]\n",
    "ax.plot(xs, ys)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "ax.scatter(support_vec['x1'], support_vec['x2'], marker='o', facecolor='none', s=200, color='red')\n",
    "sns.despine(ax=ax, left=True, bottom=True, offset=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Coding SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to use SVM for classifying the `y` value of 4-dimensional data points. The dataset has been preprocessed and splited into `svm-train.csv` and `svm-test.csv` for you.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question we are going to use the `cvxopt` package to help us solve the optimization problem of SVM. You will see it in the .py files, but you don't need to any coding with it. For this question, you only need to implement the right kernel function, and your kernel matrix `K` in `svm.py` line 135 will be pluged in the cvxopt optimization problem solver.\n",
    "\n",
    "\n",
    "For more information about `cvxopt` please refer to http://cvxopt.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (1098, 4) (1098,)\n",
      "Testing data shape: (274, 4) (274,)\n"
     ]
    }
   ],
   "source": [
    "from hw2code.svm import SVM\n",
    "svm = SVM()\n",
    "svm.load_data('./data/svm-train.csv', './data/svm-test.csv')\n",
    "# As a sanity check, we print out the size of the training data (1098, 4) and (1098,) and testing data (274, 4) and (274,)\n",
    "print('Training data shape: ', svm.train_x.shape, svm.train_y.shape)\n",
    "print('Testing data shape:', svm.test_x.shape, svm.test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Linear kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the `SVM.predict` and `linear_kernel` function in `svm.py`. \n",
    "Train a hard margin SVM and a soft margin SVM with linear kernel. Print the test accuracy for both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1098 support vectors out of 1098 points\n",
      "38 support vectors out of 1098 points\n",
      "Hard margin test accuracy is:  0.5547445255474452\n",
      "Soft margin test accuracy is:  0.9890510948905109\n"
     ]
    }
   ],
   "source": [
    "svm_hard = SVM()\n",
    "svm_hard.load_data('./data/svm-train.csv', './data/svm-test.csv')\n",
    "hard_test_acc = 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "svm_hard.train()\n",
    "hard_test_acc = svm_hard.test()\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "\n",
    "svm_soft = SVM()\n",
    "svm_soft.load_data('./data/svm-train.csv', './data/svm-test.csv')\n",
    "soft_test_acc = 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "svm_soft.train(C=1)\n",
    "soft_test_acc = svm_soft.test()\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Hard margin test accuracy is: ', hard_test_acc)\n",
    "print('Soft margin test accuracy is: ', soft_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Are these two results similar? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not similar. For hard margin SVM, it necessitates that all the data points are of certain margin to the decision boundary. However, in higher dimensions, our data cannot be separated neatly by a hyperplane. In such case, hard margin SVM uses all the points as support vectors, so it requires all the points are outside of the margin and thus it leaves little margin in between. Consequently, the small margin does not provide us with a well-separated decision boundary, and the test accuracy for hard SVM is far below the soft SVM case.\n",
    "\n",
    "On the contrary, the soft SVM allows some extent of misclassifications as well as allowing some points inside the margin. As a result, we have only 38 support vectors, leading to a larger margin and less overfitting than the hard SVM case. The larger margin gives rise to a more robust decision boundary and leads to an almost perfect test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Polynomial kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the `polynomial_kernel` function in `svm.py`. \n",
    "Train a soft margin SVM with degree 3 polynomial kernel and parameter `C = 100` for the regularization term. Print the test accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 support vectors out of 1098 points\n",
      "Test accuracy is:  0.927007299270073\n"
     ]
    }
   ],
   "source": [
    "svm = SVM()\n",
    "svm.load_data('./data/svm-train.csv', './data/svm-test.csv')\n",
    "test_acc = 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "svm.train('polynomial_kernel', C=100)\n",
    "test_acc = svm.test()\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Test accuracy is: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Is the result better than linear kernel? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer**\n",
    "Test accuracy with polynomial degree = 2: 1.0\n",
    "\n",
    "Test accuracy with polynomial degree = 3: 0.927007299270073\n",
    "\n",
    "Test accuracy with polynomial degree = 5: 0.9781021897810219\n",
    "\n",
    "We can see that if we use the polynomial of degree 2, then we have the perfect accuracy. That is to say, using polynomial of degree 3 may cause an overfit to the training data that reduces the accuracy on the testing set.\n",
    "\n",
    "For polynomials of degree 3 and above, none of them outperform the soft SVM with linear kernel. This is possibly because that the high-dimensional polynomial transformation of the data points still cannot give us a boundary that separate them clearly. Maybe it is worse than the linear case since the distribution of mapped data are more complex and thus more difficult to separate by a hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Please write down your answers and/or observations here </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Gaussian kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the `gaussian_kernel` function using the `gaussian_kernel_point` in `svm.py`. \n",
    "Train a soft margin SVM with Gaussian kernel and parameter `C = 100` for the regularization term. Print the test accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 support vectors out of 1098 points\n",
      "Test accuracy is:  1.0\n"
     ]
    }
   ],
   "source": [
    "svm = SVM()\n",
    "svm.load_data('./data/svm-train.csv', './data/svm-test.csv')\n",
    "test_acc = 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "svm.train('gaussian_kernel', C=100)\n",
    "test_acc = svm.test()\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Test accuracy is: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Is the result better than linear kernel and polynomial kernel? Why or why not?\n",
    " 2. Which one of these four models do you like the most and why?\n",
    " 3. (Bonus question, optional) Can you come up with a vectorized implementation of `gaussian_kernel` without calling `gaussian_kernel_point`? Fill that in svm.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Please write down your answers and/or observations here </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 1\n",
    "\n",
    "The result is indeed better than both the linear kernel and the polynomial kernel. As introduced in lecture, the Gaussian kernel gives us a mapping which is infinite in the dimension of x. That is to say, we can treat the Guassian kernel as a polynomial kernel with infinite dimensions. As a result, the Gaussian kernel can be generalized to different kinds of distributions, and it gives us the bell-shaped surface centered at each support vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 2\n",
    "\n",
    "I prefer the soft SVM with Gaussian kernel, since it can extend x to infinite dimensions and normalize it with its distribution. That is to say, Gaussian kernel can be well adopted to different kinds of distributions. On the contrary, linear kernel are only suitable for linear-separable data, and the polynomial kernel has limited power to expand the feature space of x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 3\n",
    "\n",
    "See the vectorized implementation in svm.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 support vectors out of 1098 points\n",
      "Test accuracy is:  1.0\n"
     ]
    }
   ],
   "source": [
    "svm = SVM()\n",
    "svm.load_data('./data/svm-train.csv', './data/svm-test.csv')\n",
    "test_acc = 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "svm.train('gaussian_kernel', C=100)\n",
    "test_acc = svm.test()\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Test accuracy is: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Homework 2 :)\n",
    "After you've finished the homework, please print out the entire `ipynb` notebook and two `py` files into one PDF file. Make sure you include the output of code cells and answers for questions. Prepare submit it to GradeScope. Also this time remember assign the pages to the questions on GradeScope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
