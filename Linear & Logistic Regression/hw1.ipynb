{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS145 Howework 1 \n",
    "\n",
    "<span style=\"color:red\"> **Important Note:** </span>\n",
    "HW1 is due on **11:59 PM PT, Oct 19 (Monday, Week 3)**. Please submit through GradeScope (you will receive an invite to Gradescope for CS145 Fall 2020.). \n",
    "\n",
    "## Print Out Your Name and UID\n",
    "\n",
    "<span style=\"color:blue\"> **Name: Rui Deng, UID: 205123245** </span>\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "You need to first create HW1 conda environment by the given `cs145hw1.yml` file, which provides the name and necessary packages for this tasks. If you have `conda` properly installed, you may create, activate or deactivate by the following commands:\n",
    "\n",
    "```\n",
    "conda env create -f cs145hw1.yml\n",
    "conda activate hw1\n",
    "conda deactivate\n",
    "```\n",
    "OR \n",
    "\n",
    "```\n",
    "conda env create --name NAMEOFYOURCHOICE -f cs145hw1.yml \n",
    "conda activate NAMEOFYOURCHOICE\n",
    "conda deactivate\n",
    "```\n",
    "To view the list of your environments, use the following command:\n",
    "```\n",
    "conda env list\n",
    "```\n",
    "\n",
    "More useful information about managing environments can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).\n",
    "\n",
    "You may also quickly review the usage of basic Python and Numpy package, if needed in coding for matrix operations.\n",
    "\n",
    "In this notebook, you must not delete any code cells in this notebook. If you change any code outside the blocks that you are allowed to edit (between `STRART/END YOUR CODE HERE`), you need to highlight these changes. You may add some additional cells to help explain your results and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sys \n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can successfully run the code above, there will be no problem for environment setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear regression \n",
    "This workbook will walk you through a linear regression example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training data shape: ', (1000, 100))\n",
      "('Training labels shape:', (1000,))\n"
     ]
    }
   ],
   "source": [
    "from hw1code.linear_regression import LinearRegression\n",
    "\n",
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "# As a sanity check, we print out the size of the training data (1000, 100) and training labels (1000,)\n",
    "print('Training data shape: ', lm.train_x.shape)\n",
    "print('Training labels shape:', lm.train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Closed form solution\n",
    "In this section, complete the `getBeta` function in `linear_regression.py` which use the close for solution of $\\hat{\\beta}$.\n",
    "\n",
    "Train you model by using `lm.train('0')` function.\n",
    "\n",
    "Print the training error and the testing error using `lm.predict` and `lm.compute_mse` given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Learning Algorithm Type: ', '0')\n",
      "('Training error is: ', 0.08693886675396784)\n",
      "('Testing error is: ', 0.110175402816758)\n"
     ]
    }
   ],
   "source": [
    "from hw1code.linear_regression import LinearRegression\n",
    "\n",
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "training_error= 0\n",
    "testing_error= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "\n",
    "beta = lm.train('0')\n",
    "predicted_train_y = lm.predict(lm.train_x, beta)\n",
    "training_error = lm.compute_mse(predicted_train_y, lm.train_y)\n",
    "predicted_test_y = lm.predict(lm.test_x, beta)\n",
    "testing_error = lm.compute_mse(predicted_test_y, lm.test_y)\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training error is: ', training_error)\n",
    "print('Testing error is: ', testing_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Batch gradient descent\n",
    "In this section, complete the `getBetaBatchGradient` function in `linear_regression.py` which compute the gradient of the objective fuction. \n",
    "\n",
    "Train you model by using `lm.train('1')` function.\n",
    "\n",
    "Print the training error and the testing error using `lm.predict` and `lm.compute_mse` given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Learning Algorithm Type: ', '1')\n",
      "('Training accuracy is: ', 0.08694023606971792)\n",
      "('Testing accuracy is: ', 0.11021514372539926)\n"
     ]
    }
   ],
   "source": [
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "training_error= 0\n",
    "testing_error= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "\n",
    "beta = lm.train('1')\n",
    "predicted_train_y = lm.predict(lm.train_x, beta)\n",
    "training_error = lm.compute_mse(predicted_train_y, lm.train_y)\n",
    "predicted_test_y = lm.predict(lm.test_x, beta)\n",
    "testing_error = lm.compute_mse(predicted_test_y, lm.test_y)\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_error)\n",
    "print('Testing accuracy is: ', testing_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Stochastic gadient descent \n",
    "In this section, complete the `getBetaStochasticGradient` function in `linear_regression.py`, which use an estimated gradient of the objective function.\n",
    "\n",
    "Train you model by using `lm.train('2')` function.\n",
    "\n",
    "Print the training error and the testing error using `lm.predict` and `lm.compute_mse` given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Learning Algorithm Type: ', '2')\n",
      "('Training accuracy is: ', 0.09862058021123295)\n",
      "('Testing accuracy is: ', 0.11634315938682402)\n"
     ]
    }
   ],
   "source": [
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "training_error= 0\n",
    "testing_error= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "\n",
    "beta = lm.train('2')\n",
    "predicted_train_y = lm.predict(lm.train_x, beta)\n",
    "training_error = lm.compute_mse(predicted_train_y, lm.train_y)\n",
    "predicted_test_y = lm.predict(lm.test_x, beta)\n",
    "testing_error = lm.compute_mse(predicted_test_y, lm.test_y)\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_error)\n",
    "print('Testing accuracy is: ', testing_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "1. Compare the MSE on the testing dataset for each version. Are they the same? Why or why not?\n",
    "2. Apply z-score normalization for eachh featrure and comment whether or not it affect the three algorithm. \n",
    "3. Ridge regression is adding an L2 regularization term to the original objective function of mean squared error. The objective function become following: \n",
    "    $$ J(\\beta) = \\frac{1}{2n} \\sum_i \\left(x_i^T\\beta - y_i \\right)^2 + \\frac{\\lambda}{2n} \\sum_j \\beta_j^2 ,$$ \n",
    "where $\\lambda \\geq 0$, which is a hyper parameter that controls the trade off. Take the derivative of this provided objective function and derive the closed form solution for $\\beta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Please type your answer here! </span>\n",
    "1. MSE \n",
    "for closed form solution: 0.110175402816758\n",
    "\n",
    "for batch gradient descent: 0.11021514372539926\n",
    "\n",
    "for stochastic gradient descent (with learning rate 0.0005): 0.11634315938682402\n",
    "\n",
    "They are not the same. For closed form solution, the MSE is the same no matter how many times we run the algorithm. This is because we are calculating beta from a pre-defined formula and there is no randomness in closed form solution. However, there are small differences in MSE if we run the gradient descent method for multiple times, since we initialize beta randomly and also use random mini-batch for stochastic gradient descent. Thus, with such randomness, MSE from the last two methods cannot be the same with MSE in closed form solution.\n",
    "\n",
    "Still, all of the three methods give us similar error, since they all converge to the optimal solution.\n",
    "\n",
    "2. After normalization:\n",
    "for closed form solution: 0.110175402816758\n",
    "\n",
    "for batch gradient descent: 0.13744440164075847\n",
    "\n",
    "for stochastic gradient descent (with learning rate 0.0005): 0.13721370635089097\n",
    "\n",
    "Normalization does not change the MSE for closed form solution, but it does increase the MSE of testing set for both gradient descent methods. This is because we are using the mean and standard deviation of the training set to normalize the testing set, and the statistics for the training set may not be appropriate for the testing set.\n",
    "\n",
    "3. Derivative:\n",
    "\n",
    "$$ \\frac{\\partial J(\\beta)}{\\partial \\beta} = \\frac{1}{n} \\left( \\sum_i \\left(x_i^T\\beta - y_i \\right) x_i + \\lambda \\sum_j \\beta_j \\right) $$\n",
    "\n",
    "If we write this in vector form, we have:\n",
    "$$ \\frac{\\partial J(\\beta)}{\\partial \\beta} = \\frac{1}{n} \\left( X^T X \\beta - X^T y + \\lambda \\beta \\right) $$\n",
    "\n",
    "Set this derivative to 0, and we get:\n",
    "\n",
    "$$ \\beta = \\left(X^T X + \\lambda I \\right)^{-1} X^T y $$\n",
    "\n",
    "as our new closed form solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic regression \n",
    "This workbook will walk you through a logistic regression example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training data shape: ', (1000, 5))\n",
      "('Training labels shape:', (1000,))\n"
     ]
    }
   ],
   "source": [
    "from hw1code.logistic_regression import LogisticRegression\n",
    "\n",
    "lm=LogisticRegression()\n",
    "lm.load_data('./data/logistic-regression-train.csv','./data/logistic-regression-test.csv')\n",
    "# As a sanity chech, we print out the size of the training data (1000, 5) and training labels (1000,)\n",
    "print('Training data shape: ', lm.train_x.shape)\n",
    "print('Training labels shape:', lm.train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Batch gradiend descent\n",
    "In this section, complete the `getBeta_BatchGradient` in `logistic_regression.py`, which compute the gradient of the log likelihoood function. \n",
    "\n",
    "Complete the `compute_avglogL` function in `logistic_regression.py` for sanity check. \n",
    "\n",
    "Train you model by using `lm.train('0')` function.\n",
    "\n",
    "And print the training and testing accuracy using `lm.predict` and `lm.compute_accuracy` given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average logL for iteration 0: -0.575495420086 \t\n",
      "average logL for iteration 1000: -0.504071168498 \t\n",
      "average logL for iteration 2000: -0.484364560378 \t\n",
      "average logL for iteration 3000: -0.475661811496 \t\n",
      "average logL for iteration 4000: -0.47087006589 \t\n",
      "average logL for iteration 5000: -0.467925360099 \t\n",
      "average logL for iteration 6000: -0.465993392131 \t\n",
      "average logL for iteration 7000: -0.46466668148 \t\n",
      "average logL for iteration 8000: -0.463722801457 \t\n",
      "average logL for iteration 9000: -0.463031297563 \t\n",
      "('Training avgLogL: ', -0.46251212811749887)\n",
      "('Training accuracy is: ', 0.8)\n",
      "('Testing accuracy is: ', 0.7415506958250497)\n"
     ]
    }
   ],
   "source": [
    "lm=LogisticRegression()\n",
    "lm.load_data('./data/logistic-regression-train.csv','./data/logistic-regression-test.csv')\n",
    "training_accuracy= 0\n",
    "testing_accuracy= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "beta = lm.train('0')\n",
    "predicted_train_y = lm.predict(lm.train_x, beta)\n",
    "training_accuracy = lm.compute_accuracy(predicted_train_y, lm.train_y)\n",
    "predicted_test_y = lm.predict(lm.test_x, beta)\n",
    "testing_accuracy = lm.compute_accuracy(predicted_test_y, lm.test_y)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_accuracy)\n",
    "print('Testing accuracy is: ', testing_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Newton Raphhson\n",
    "In this section, complete the `getBeta_Newton` in `logistic_regression.py`, which make use of both first and second derivative.\n",
    "\n",
    "Train you model by using `lm.train('1')` function.\n",
    "\n",
    "Print the training and testing accuracy using `lm.predict` and `lm.compute_accuracy` given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average logL for iteration 0: -0.620851256358 \t\n",
      "average logL for iteration 500: -0.527671233888 \t\n",
      "average logL for iteration 1000: -0.489163287119 \t\n",
      "average logL for iteration 1500: -0.472294578524 \t\n",
      "average logL for iteration 2000: -0.465045797813 \t\n",
      "average logL for iteration 2500: -0.46204601611 \t\n",
      "average logL for iteration 3000: -0.460848349572 \t\n",
      "average logL for iteration 3500: -0.460383332502 \t\n",
      "average logL for iteration 4000: -0.460206285828 \t\n",
      "average logL for iteration 4500: -0.460139749201 \t\n",
      "average logL for iteration 5000: -0.460114951079 \t\n",
      "average logL for iteration 5500: -0.460105756944 \t\n",
      "average logL for iteration 6000: -0.460102359119 \t\n",
      "average logL for iteration 6500: -0.460101105889 \t\n",
      "average logL for iteration 7000: -0.460100644216 \t\n",
      "average logL for iteration 7500: -0.460100474267 \t\n",
      "average logL for iteration 8000: -0.460100411734 \t\n",
      "average logL for iteration 8500: -0.460100388732 \t\n",
      "average logL for iteration 9000: -0.460100380272 \t\n",
      "average logL for iteration 9500: -0.46010037716 \t\n",
      "('Training avgLogL: ', -0.46010037601754294)\n",
      "('Training accuracy is: ', 0.797)\n",
      "('Testing accuracy is: ', 0.7534791252485089)\n"
     ]
    }
   ],
   "source": [
    "lm=LogisticRegression()\n",
    "lm.load_data('./data/logistic-regression-train.csv','./data/logistic-regression-test.csv')\n",
    "training_accuracy= 0\n",
    "testing_accuracy= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "beta = lm.train('1')\n",
    "predicted_train_y = lm.predict(lm.train_x, beta)\n",
    "training_accuracy = lm.compute_accuracy(predicted_train_y, lm.train_y)\n",
    "predicted_test_y = lm.predict(lm.test_x, beta)\n",
    "testing_accuracy = lm.compute_accuracy(predicted_test_y, lm.test_y)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_accuracy)\n",
    "print('Testing accuracy is: ', testing_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "1. Compare the accuracy on the testing dataset for each version. Are they the same? Why or why not?\n",
    "2. Regularization. Similar to linear regression, an regularization term could be added to logistic regression. \n",
    "The objective function becomes following: \n",
    "    $$ J(\\beta) = -\\frac{1}{n} \\sum_i \\left(y_i x_i^T \\beta - \\log \\left( 1+ \\exp\\{ x_i^T \\beta \\} \\right) \\right) + \\lambda \\sum_j \\beta_j^2,$$ \n",
    "where $\\lambda \\geq 0$, which is a hyper parameter that controls the trade off. Take the derivative $\\frac{\\partial J(\\beta)}{\\partial \\beta_j}$ of this provided objective function and provide the batch gradient descent update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Please type your answer here! </span>\n",
    "1. Testing accuracy\n",
    "for gradient descent: 0.7415506958250497\n",
    "for Newton Raphson: 0.7534791252485089\n",
    "Newton Raphson has slightly higher accuracy than batch gradient descent, probably because that it adjusts learning rate throughout the optimization process through the hessian matrix instead of using a pre-defined learning rate.\n",
    "\n",
    "2. Derivative\n",
    "$$ \\frac{\\partial J(\\beta)}{\\partial \\beta_j} = - \\frac{1}{n} \\sum_i x_{ij} \\left(y_i - p_i(\\beta) \\right) + 2\\lambda \\beta_j $$,\n",
    "and we define $p_i$ as\n",
    "$$p_i = \\frac {\\exp\\{ x_i^T \\beta \\}} {1 + \\exp\\{ x_i^T \\beta \\}} $$\n",
    "Then, we have the update rule as:\n",
    "$$ \\beta^{new} = \\beta^{old}  +\\frac{\\eta}{n}  \\sum_i x_{i} \\left(y_i - p_i(\\beta) \\right) + 2\\eta \\lambda \\beta^{old} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualize the decision boundary on a toy dataset\n",
    "\n",
    "In this subsection, you will use the same implementation for another small dataset with each datapoint $x$ with only two features $(x_1, x_2)$ to visualize the decision boundary of logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training data shape: ', (99, 2))\n",
      "('Training labels shape:', (99,))\n"
     ]
    }
   ],
   "source": [
    "from hw1code.logistic_regression import LogisticRegression\n",
    "\n",
    "lm=LogisticRegression(verbose = False)\n",
    "lm.load_data('./data/logistic-regression-toy.csv','./data/logistic-regression-toy.csv')\n",
    "# As a sanity chech, we print out the size of the training data (99,2) and training labels (99,)\n",
    "print('Training data shape: ', lm.train_x.shape)\n",
    "print('Training labels shape:', lm.train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block, you can apply the same implementation of logistic regression model (either in 2.1 or 2.2) to the toy dataset. Print out the $\\hat{\\beta}$ after training and accuracy on the train set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training avgLogL: ', -0.32937413231151724)\n",
      "[-0.04119331  1.41642477  1.97907353]\n",
      "('Training accuracy is: ', 0.8888888888888888)\n"
     ]
    }
   ],
   "source": [
    "training_accuracy= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "beta = lm.train('0')\n",
    "print(beta)\n",
    "predicted_train_y = lm.predict(lm.train_x, beta)\n",
    "training_accuracy = lm.compute_accuracy(predicted_train_y, lm.train_y)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we try to plot the decision boundary of your learned logistic regression classifier. Generally, a decision boundary is the region of a space in which the output label of a classifier is ambiguous. That is, in the given toy data, given a datapoint $x=(x_1, x_2)$ on the decision boundary, the logistic regression classifier cannot decide whether $y=0$ or $y=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Is the decision boundary for logistic regression linear? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Please type your answer here! </span>\n",
    "\n",
    "The boundary is linear because if $X\\beta$ > 0, our sigmoid function will give us probability > 0.5 thus belong to label 1; and vice versa. Thus, our decision boundary is $X\\beta$ = 0, which is a hyperplane in higher dimension and a line in 2D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the decision boundary in the following cell. Note that the code to plot the raw data points are given. You may need `plt.plot` function (see [here](https://matplotlib.org/tutorials/introductory/pyplot.html)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scatter plot the raw data\n",
    "df = pd.concat([lm.train_x, lm.train_y], axis=1)\n",
    "groups = df.groupby(\"y\")\n",
    "for name, group in groups:\n",
    "    plt.plot(group[\"x1\"], group[\"x2\"], marker=\"o\", linestyle=\"\", label=name)\n",
    "    \n",
    "# plot the decision boundary on top of the scattered points\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "xs = np.linspace(-2, 2, 100)\n",
    "ys = - beta[1] / beta[2] * xs - beta[0] / beta[2]\n",
    "plt.plot(xs, ys)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Homework 1 :)\n",
    "After you've finished the homework, please print out the entire `ipynb` notebook and two `py` files into one PDF file. Make sure you include the output of code cells and answers for questions. Prepare submit it to GradeScope. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
