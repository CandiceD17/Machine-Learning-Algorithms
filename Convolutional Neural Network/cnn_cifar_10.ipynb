{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_cifar-10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu00PGeDHcDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_dLvmChIFGF",
        "colab_type": "text"
      },
      "source": [
        "Define functions for data augmentation of training set.\n",
        "If we encounter RGB( 3 channels ) images , we will use the following to normalize:\n",
        "```\n",
        "transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                     std=[0.229, 0.224, 0.225])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVnC5ZR9Hzrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "                                      transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                           std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                           std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PYYCBNKLFt1",
        "colab_type": "text"
      },
      "source": [
        "Load the CIFAR-10 Dataset by downloading it with torchvision's datasets function.\n",
        "\n",
        "torch.utils.data.DataLoader class represents a Python iterable over a dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HIu6jRaLPdh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5eadf595-f042-4753-e153-06fa2fd420ef"
      },
      "source": [
        "opt_batch = 16\n",
        "trainset = torchvision.datasets.CIFAR10(root='.', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(dataset=trainset, batch_size=opt_batch, shuffle=True, num_workers=2)\n",
        "testset = torchvision.datasets.CIFAR10(root='.', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(dataset=testset, batch_size=opt_batch, shuffle=True, num_workers=2)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1uIuMsJQDVj",
        "colab_type": "text"
      },
      "source": [
        "Define a cnn model here. (I copied the parameters directly from other's model.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNryheveQGPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CNN(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "\n",
        "    self.conv_layer = nn.Sequential(\n",
        "        #block 1\n",
        "        nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "        #block 2\n",
        "        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=0.05),\n",
        "\n",
        "        #block 3\n",
        "        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "\n",
        "    self.fc_layer = nn.Sequential(\n",
        "        nn.Dropout(p=0.1),\n",
        "        nn.Linear(4096, 1024),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(1024, 512),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(p=0.1),\n",
        "        nn.Linear(512, 10)\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.conv_layer(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.fc_layer(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "net = CNN()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIZQZ3SnkBOY",
        "colab_type": "text"
      },
      "source": [
        "Define a loss function using cross-entropy loss (for classification) and Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkqCk9fnkKfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nPGA_tWk510",
        "colab_type": "text"
      },
      "source": [
        "Train & Test the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgN7g1kslDtj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "d40808d5-9f82-4cc5-be2e-896c7389fabd"
      },
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "def calculate_accuracy(loader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  accur = 0.0\n",
        "  with torch.no_grad():\n",
        "    for data in loader:\n",
        "      images, labels = data\n",
        "      outputs = net(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "  accur = correct / total * 100\n",
        "  return accur\n",
        "\n",
        "epoch_size = 50\n",
        "\n",
        "for epoch in range(epoch_size):\n",
        "  running_loss = 0.0\n",
        "  for i,data in enumerate(trainloader, 0):\n",
        "    inputs, labels = data\n",
        "    inputs, labels = Variable(inputs), Variable(labels)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "\n",
        "    if epoch > 16:\n",
        "      for group in optimizer.param_groups:\n",
        "        for p in group['params']:\n",
        "          state = optimizer.state[p]\n",
        "          if state['step'] >= 1024:\n",
        "            state['step'] = 1000\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "  \n",
        "  running_loss /= len(trainloader)\n",
        "  train_accur = calculate_accuracy(trainloader)\n",
        "  test_accur = calculate_accuracy(testloader)\n",
        "\n",
        "  print(\"Iteration: {0} | Loss: {1} | Training accuracy: {2}% | Test accuracy: {3}%\".format(epoch+1, running_loss, train_accur, test_accur))\n",
        "\n",
        "print('==> Finished Training ...')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 1 | Loss: 1.0925072313404083 | Training accuracy: 67.794% | Test accuracy: 66.16%\n",
            "Iteration: 2 | Loss: 0.9023744855165482 | Training accuracy: 70.666% | Test accuracy: 70.82000000000001%\n",
            "Iteration: 3 | Loss: 0.8017339448261261 | Training accuracy: 75.486% | Test accuracy: 73.11%\n",
            "Iteration: 4 | Loss: 0.7297243852877617 | Training accuracy: 76.58% | Test accuracy: 74.15%\n",
            "Iteration: 5 | Loss: 0.6839256379318237 | Training accuracy: 78.018% | Test accuracy: 77.38000000000001%\n",
            "Iteration: 6 | Loss: 0.6390557837605476 | Training accuracy: 80.502% | Test accuracy: 77.97%\n",
            "Iteration: 7 | Loss: 0.6065029391860962 | Training accuracy: 82.33200000000001% | Test accuracy: 79.03999999999999%\n",
            "Iteration: 8 | Loss: 0.5654581157708168 | Training accuracy: 81.91199999999999% | Test accuracy: 78.35%\n",
            "Iteration: 9 | Loss: 0.5463207822176814 | Training accuracy: 82.974% | Test accuracy: 79.79%\n",
            "Iteration: 10 | Loss: 0.5242690891569853 | Training accuracy: 83.908% | Test accuracy: 79.61%\n",
            "Iteration: 11 | Loss: 0.5038607841491699 | Training accuracy: 84.08200000000001% | Test accuracy: 80.42%\n",
            "Iteration: 12 | Loss: 0.49423466526687143 | Training accuracy: 85.134% | Test accuracy: 81.17999999999999%\n",
            "Iteration: 13 | Loss: 0.4753914400434494 | Training accuracy: 85.636% | Test accuracy: 82.17%\n",
            "Iteration: 14 | Loss: 0.45912314891844985 | Training accuracy: 85.346% | Test accuracy: 81.38%\n",
            "Iteration: 15 | Loss: 0.45185967353105544 | Training accuracy: 85.056% | Test accuracy: 81.28999999999999%\n",
            "Iteration: 16 | Loss: 0.43519172457158567 | Training accuracy: 86.598% | Test accuracy: 82.47%\n",
            "Iteration: 17 | Loss: 0.4276999799120426 | Training accuracy: 86.706% | Test accuracy: 82.56%\n",
            "Iteration: 18 | Loss: 0.39134473053455354 | Training accuracy: 87.012% | Test accuracy: 83.03%\n",
            "Iteration: 19 | Loss: 0.3759314465966821 | Training accuracy: 87.92% | Test accuracy: 83.86%\n",
            "Iteration: 20 | Loss: 0.36242746270328763 | Training accuracy: 88.786% | Test accuracy: 84.17999999999999%\n",
            "Iteration: 21 | Loss: 0.35861855942323806 | Training accuracy: 86.44800000000001% | Test accuracy: 82.07%\n",
            "Iteration: 22 | Loss: 0.3535864471217245 | Training accuracy: 89.05% | Test accuracy: 83.77%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}